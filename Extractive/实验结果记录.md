# 实验结果记录

## no-middle

```sh
python run_coqa.py --model_type bert \
                   --model_name_or_path ./bert-base-chinese \
                   --do_train \
                   --do_eval \
                   --data_dir data/raw_data_cn_subdomain \
                   --train_file ConvQA_CN_v3.0_train.json \
                   --predict_file ConvQA_CN_v3.0_dev.json \
                   --max_seq_length 512 \
                   --doc_stride 384 \
                   --learning_rate 3e-5 \
                   --num_train_epochs 3 \
                   --output_dir bert-output-v3/ \
                   --do_lower_case \
                   --per_gpu_train_batch_size 32  \
                   --per_gpu_eval_batch_size 128 \
                   --gradient_accumulation_steps 1 \
                   --max_grad_norm -1 \
                   --threads 4 \
                   --weight_decay 0.01 \
```

#### 日志，总的$F_1$得分为34.3（5个epoch的话，过拟合严重）

```shell
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Training/evaluation parameters Namespace(adam_epsilon=1e-08, adversarial=False, cache_dir='', config_name='', data_dir='data/raw_data_cn_subdomain', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, doc_stride=384, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, history_len=2, learning_rate=3e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=-1.0, max_query_length=64, max_seq_length=512, max_steps=-1, model_name_or_path='./bert-base-chinese', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='bert-output-v3/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=128, per_gpu_train_batch_size=32, predict_file='ConvQA_CN_v3.0_dev.json', save_steps=5000, seed=42, server_ip='', server_port='', threads=4, tokenizer_name='', train_file='ConvQA_CN_v3.0_train.json', verbose_logging=False, warmup_steps=2000, weight_decay=0.01)
Total parameter number: 104634630
Creating features from dataset file at data/raw_data_cn_subdomain
Saving features into cached file data/raw_data_cn_subdomain/cached_train_bert-base-chinese_512
***** Running training *****
  Num examples = 54054
  Num Epochs = 3
  Instantaneous batch size per GPU = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 5070
  Starting fine-tuning.
Step: 50	Learning rate: 7.350000000000001e-07	Loss: 5.968645820617676	
Step: 100	Learning rate: 1.4850000000000002e-06	Loss: 5.7122864913940425	
Step: 150	Learning rate: 2.2349999999999998e-06	Loss: 5.082963914871216	
Step: 200	Learning rate: 2.9850000000000002e-06	Loss: 3.9031320142745973	
Step: 250	Learning rate: 3.7350000000000002e-06	Loss: 2.8790844821929933	
Step: 300	Learning rate: 4.485e-06	Loss: 2.460149955749512	
Step: 350	Learning rate: 5.235e-06	Loss: 2.2544813203811644	
Step: 400	Learning rate: 5.985e-06	Loss: 2.3329155683517455	
Step: 450	Learning rate: 6.735000000000001e-06	Loss: 2.308885476589203	
Step: 500	Learning rate: 7.485e-06	Loss: 2.28866792678833	
Step: 550	Learning rate: 8.235000000000002e-06	Loss: 2.2434185099601747	
Step: 600	Learning rate: 8.985e-06	Loss: 2.0958262372016905	
Step: 650	Learning rate: 9.735e-06	Loss: 2.0157574367523194	
Step: 700	Learning rate: 1.0485e-05	Loss: 1.768771517276764	
Step: 750	Learning rate: 1.1235e-05	Loss: 1.7794734358787536	
Step: 800	Learning rate: 1.1985000000000001e-05	Loss: 1.6825894594192505	
Step: 850	Learning rate: 1.2735e-05	Loss: 1.6010856902599335	
Step: 900	Learning rate: 1.3485e-05	Loss: 1.5153931665420532	
Step: 950	Learning rate: 1.4235e-05	Loss: 1.5892179322242737	
Step: 1000	Learning rate: 1.4985e-05	Loss: 1.5418814468383788	
Step: 1050	Learning rate: 1.5735e-05	Loss: 1.3712692618370057	
Step: 1100	Learning rate: 1.6485e-05	Loss: 1.4650654149055482	
Step: 1150	Learning rate: 1.7235e-05	Loss: 1.3609683907032013	
Step: 1200	Learning rate: 1.7985e-05	Loss: 1.3660607647895813	
Step: 1250	Learning rate: 1.8735000000000003e-05	Loss: 1.3490563428401947	
Step: 1300	Learning rate: 1.9485e-05	Loss: 1.2995196497440338	
Step: 1350	Learning rate: 2.0235e-05	Loss: 1.204262557029724	
Step: 1400	Learning rate: 2.0985e-05	Loss: 1.231541439294815	
Step: 1450	Learning rate: 2.1735e-05	Loss: 1.160599479675293	
Step: 1500	Learning rate: 2.2485000000000002e-05	Loss: 1.1637792885303497	
Step: 1550	Learning rate: 2.3235e-05	Loss: 1.1034841978549956	
Step: 1600	Learning rate: 2.3985e-05	Loss: 1.2016894793510438	
Step: 1650	Learning rate: 2.4735e-05	Loss: 1.0856788837909699	
Step: 1700	Learning rate: 2.5485e-05	Loss: 1.1529019737243653	
Step: 1750	Learning rate: 2.6235000000000002e-05	Loss: 1.062381420135498	
Step: 1800	Learning rate: 2.6985e-05	Loss: 1.0991289746761321	
Step: 1850	Learning rate: 2.7735e-05	Loss: 1.004460220336914	
Step: 1900	Learning rate: 2.8485000000000003e-05	Loss: 1.041058624982834	
Step: 1950	Learning rate: 2.9235e-05	Loss: 1.0947954618930817	
Step: 2000	Learning rate: 2.9985000000000002e-05	Loss: 1.0553790831565857	
Step: 2050	Learning rate: 2.9521172638436482e-05	Loss: 1.0226871657371521	
Step: 2100	Learning rate: 2.903257328990228e-05	Loss: 1.0024821293354034	
Step: 2150	Learning rate: 2.854397394136808e-05	Loss: 0.989396549463272	
Step: 2200	Learning rate: 2.8055374592833875e-05	Loss: 1.0236723697185517	
Step: 2250	Learning rate: 2.7566775244299674e-05	Loss: 1.0026826906204223	
Step: 2300	Learning rate: 2.7078175895765473e-05	Loss: 0.9721248304843902	
Step: 2350	Learning rate: 2.658957654723127e-05	Loss: 0.9909070444107055	
Step: 2400	Learning rate: 2.6100977198697067e-05	Loss: 1.0214431583881378	
Step: 2450	Learning rate: 2.5612377850162865e-05	Loss: 0.9931621658802032	
Step: 2500	Learning rate: 2.5123778501628664e-05	Loss: 1.0088956725597382	
Step: 2550	Learning rate: 2.4635179153094466e-05	Loss: 0.8955438667535782	
Step: 2600	Learning rate: 2.414657980456026e-05	Loss: 0.9041083228588104	
Step: 2650	Learning rate: 2.365798045602606e-05	Loss: 0.9339950740337372	
Step: 2700	Learning rate: 2.316938110749186e-05	Loss: 0.9047358250617981	
Step: 2750	Learning rate: 2.2680781758957654e-05	Loss: 0.911564701795578	
Step: 2800	Learning rate: 2.2192182410423453e-05	Loss: 0.8716369462013245	
Step: 2850	Learning rate: 2.1703583061889252e-05	Loss: 0.8646990084648132	
Step: 2900	Learning rate: 2.121498371335505e-05	Loss: 0.9302220904827118	
Step: 2950	Learning rate: 2.0726384364820846e-05	Loss: 0.9261581802368164	
Step: 3000	Learning rate: 2.0237785016286645e-05	Loss: 0.9061820149421692	
Step: 3050	Learning rate: 1.9749185667752443e-05	Loss: 0.9447636818885803	
Step: 3100	Learning rate: 1.9260586319218242e-05	Loss: 0.931940546631813	
Step: 3150	Learning rate: 1.8771986970684038e-05	Loss: 0.927426187992096	
Step: 3200	Learning rate: 1.8283387622149836e-05	Loss: 0.9461277902126313	
Step: 3250	Learning rate: 1.7794788273615635e-05	Loss: 0.8714886462688446	
Step: 3300	Learning rate: 1.7306188925081434e-05	Loss: 0.8955784773826599	
Step: 3350	Learning rate: 1.6817589576547232e-05	Loss: 0.8933060812950134	
Step: 3400	Learning rate: 1.632899022801303e-05	Loss: 0.8318181711435318	
Step: 3450	Learning rate: 1.584039087947883e-05	Loss: 0.6507020464539528	
Step: 3500	Learning rate: 1.5351791530944625e-05	Loss: 0.6841546547412872	
Step: 3550	Learning rate: 1.4863192182410424e-05	Loss: 0.6940052562952042	
Step: 3600	Learning rate: 1.4374592833876223e-05	Loss: 0.7286067134141923	
Step: 3650	Learning rate: 1.388599348534202e-05	Loss: 0.748015216588974	
Step: 3700	Learning rate: 1.3397394136807818e-05	Loss: 0.6658273988962173	
Step: 3750	Learning rate: 1.2908794788273615e-05	Loss: 0.6774388247728348	
Step: 3800	Learning rate: 1.2420195439739414e-05	Loss: 0.6481604748964309	
Step: 3850	Learning rate: 1.1931596091205211e-05	Loss: 0.6691671252250672	
Step: 3900	Learning rate: 1.144299674267101e-05	Loss: 0.6056084299087524	
Step: 3950	Learning rate: 1.0954397394136809e-05	Loss: 0.6733827275037766	
Step: 4000	Learning rate: 1.0465798045602606e-05	Loss: 0.6577873176336289	
Step: 4050	Learning rate: 9.977198697068404e-06	Loss: 0.6617413181066513	
Step: 4100	Learning rate: 9.488599348534201e-06	Loss: 0.6923719757795334	
Step: 4150	Learning rate: 9e-06	Loss: 0.6884115940332413	
Step: 4200	Learning rate: 8.511400651465797e-06	Loss: 0.6215851598978043	
Step: 4250	Learning rate: 8.022801302931596e-06	Loss: 0.6269158852100373	
Step: 4300	Learning rate: 7.534201954397395e-06	Loss: 0.6314880937337876	
Step: 4350	Learning rate: 7.045602605863192e-06	Loss: 0.6584274357557297	
Step: 4400	Learning rate: 6.5570032573289905e-06	Loss: 0.6292890131473541	
Step: 4450	Learning rate: 6.068403908794788e-06	Loss: 0.627823885679245	
Step: 4500	Learning rate: 5.579804560260586e-06	Loss: 0.6132605314254761	
Step: 4550	Learning rate: 5.091205211726384e-06	Loss: 0.6641274309158325	
Step: 4600	Learning rate: 4.602605863192183e-06	Loss: 0.6496741360425949	
Step: 4650	Learning rate: 4.114006514657981e-06	Loss: 0.6169511705636979	
Step: 4700	Learning rate: 3.6254071661237786e-06	Loss: 0.6278882527351379	
Step: 4750	Learning rate: 3.1368078175895765e-06	Loss: 0.5733664774894714	
Step: 4800	Learning rate: 2.648208469055375e-06	Loss: 0.6007771205902099	
Step: 4850	Learning rate: 2.1596091205211727e-06	Loss: 0.5891564410924911	
Step: 4900	Learning rate: 1.6710097719869708e-06	Loss: 0.5802301394939423	
Step: 4950	Learning rate: 1.1824104234527687e-06	Loss: 0.5980773928761483	
Step: 5000	Learning rate: 6.938110749185669e-07	Loss: 0.5799632638692855	
Saving model checkpoint to bert-output-v3/checkpoint-5000
Saving optimizer and scheduler states to bert-output-v3/checkpoint-5000
Step: 5050	Learning rate: 2.0521172638436482e-07	Loss: 0.6036708807945251	
 global_step = 5071, average loss = 1.229765649596231
Saving model to bert-output-v3/
Loading checkpointlogger.setLevel(logging.DEBUG)s saved during training for evaluation
Evaluate the following checkpoints: ['bert-output-v3/']
Creating features from dataset file at data/raw_data_cn_subdomain
Saving features into cached file data/raw_data_cn_subdomain/cached_dev_bert-base-chinese_512
***** Running evaluation  *****
  Num examples = 13405
  Batch size = 128
  Evaluation done in total 139.177886 secs (0.010383 sec per example)
Results: {'儿童故事': {'em': 8.9, 'f1': 27.8, 'turns': 3213}, '热点新闻': {'em': 13.9, 'f1': 54.7, 'turns': 813}, '历史': {'em': 13.9, 'f1': 45.0, 'turns': 1168}, '育儿知识': {'em': 0.3, 'f1': 8.1, 'turns': 310}, 'in_domain': {'em': 10.2, 'f1': 34.3, 'turns': 5504}, 'out_domain': {'em': 0.0, 'f1': 0.0, 'turns': 0}, 'overall': {'em': 10.2, 'f1': 34.3, 'turns': 5504}}
```

## middle

### BiLSTM

```shell
python run_coqa.py --model_type bert \
                   --model_name_or_path /home/aistudio/bert_coqa/transformers-coqa-cn/bert-base-chinese \
                   --do_train \
                   --do_eval \
                   --data_dir data/raw_data_cn_v3_sub \
                   --train_file ConvQA_CN_v3.0_train.json \
                   --predict_file ConvQA_CN_v3.0_dev.json \
                   --max_seq_length 512 \
                   --doc_stride 384 \
                   --learning_rate 3e-5 \
                   --num_train_epochs 3 \
                   --output_dir bert-output-bilstm/ \
                   --do_lower_case \
                   --per_gpu_train_batch_size 32  \
                   --per_gpu_eval_batch_size 128 \
                   --gradient_accumulation_steps 1 \
                   --max_grad_norm -1 \
                   --threads 4 \
                   --weight_decay 0.01
```

#### 日志，总的$F_1$得分为32.9

```shell
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Training/evaluation parameters Namespace(adam_epsilon=1e-08, adversarial=False, cache_dir='', config_name='', data_dir='data/raw_data_cn_v3_sub', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, doc_stride=384, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, history_len=2, learning_rate=3e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=-1.0, max_query_length=64, max_seq_length=512, max_steps=-1, model_name_or_path='/home/aistudio/bert_coqa/transformers-coqa-cn/bert-base-chinese', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='bert-output-bilstm/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=256, per_gpu_train_batch_size=32, predict_file='ConvQA_CN_v3.0_dev.json', save_steps=5000, seed=42, server_ip='', server_port='', threads=4, tokenizer_name='', train_file='ConvQA_CN_v3.0_train.json', verbose_logging=False, warmup_steps=2000, weight_decay=0.01)
Total parameter number: 109299207
Creating features from dataset file at data/raw_data_cn_v3_sub
Saving features into cached file data/raw_data_cn_v3_sub/cached_train_bert-base-chinese_512
***** Running training *****
  Num examples = 54054
  Num Epochs = 3
  Instantaneous batch size per GPU = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 5070
  Starting fine-tuning.
Step: 50	Learning rate: 7.350000000000001e-07	Loss: 5.913874998092651	
Step: 100	Learning rate: 1.4850000000000002e-06	Loss: 5.687400178909302	
Step: 150	Learning rate: 2.2349999999999998e-06	Loss: 5.093085374832153	
Step: 200	Learning rate: 2.9850000000000002e-06	Loss: 4.1598512649536135	
Step: 250	Learning rate: 3.7350000000000002e-06	Loss: 3.2131056594848633	
Step: 300	Learning rate: 4.485e-06	Loss: 2.605064845085144	
Step: 350	Learning rate: 5.235e-06	Loss: 2.349830832481384	
Step: 400	Learning rate: 5.985e-06	Loss: 2.4272649002075197	
Step: 450	Learning rate: 6.735000000000001e-06	Loss: 2.4244036293029785	
Step: 500	Learning rate: 7.485e-06	Loss: 2.331014139652252	
Step: 550	Learning rate: 8.235000000000002e-06	Loss: 2.2293209862709045	
Step: 600	Learning rate: 8.985e-06	Loss: 2.0619257521629333	
Step: 650	Learning rate: 9.735e-06	Loss: 1.988394112586975	
Step: 700	Learning rate: 1.0485e-05	Loss: 1.8251073837280274	
Step: 750	Learning rate: 1.1235e-05	Loss: 1.840022714138031	
Step: 800	Learning rate: 1.1985000000000001e-05	Loss: 1.8348823595046997	
Step: 850	Learning rate: 1.2735e-05	Loss: 1.7262251687049865	
Step: 900	Learning rate: 1.3485e-05	Loss: 1.5898954272270203	
Step: 950	Learning rate: 1.4235e-05	Loss: 1.5991987204551696	
Step: 1000	Learning rate: 1.4985e-05	Loss: 1.5513660788536072	
Step: 1050	Learning rate: 1.5735e-05	Loss: 1.3483359134197235	
Step: 1100	Learning rate: 1.6485e-05	Loss: 1.436051412820816	
Step: 1150	Learning rate: 1.7235e-05	Loss: 1.3453671193122865	
Step: 1200	Learning rate: 1.7985e-05	Loss: 1.3568315124511718	
Step: 1250	Learning rate: 1.8735000000000003e-05	Loss: 1.328597011566162	
Step: 1300	Learning rate: 1.9485e-05	Loss: 1.3266252636909486	
Step: 1350	Learning rate: 2.0235e-05	Loss: 1.2137714231014252	
Step: 1400	Learning rate: 2.0985e-05	Loss: 1.2595081579685212	
Step: 1450	Learning rate: 2.1735e-05	Loss: 1.1800959944725036	
Step: 1500	Learning rate: 2.2485000000000002e-05	Loss: 1.1943407452106476	
Step: 1550	Learning rate: 2.3235e-05	Loss: 1.116613986492157	
Step: 1600	Learning rate: 2.3985e-05	Loss: 1.1917280042171479	
Step: 1650	Learning rate: 2.4735e-05	Loss: 1.0953681111335754	
Step: 1700	Learning rate: 2.5485e-05	Loss: 1.1726891922950744	
Step: 1750	Learning rate: 2.6235000000000002e-05	Loss: 1.066633859872818	
Step: 1800	Learning rate: 2.6985e-05	Loss: 1.117080352306366	
Step: 1850	Learning rate: 2.7735e-05	Loss: 1.0947993409633636	
Step: 1900	Learning rate: 2.8485000000000003e-05	Loss: 1.0282999432086946	
Step: 1950	Learning rate: 2.9235e-05	Loss: 1.1045217680931092	
Step: 2000	Learning rate: 2.9985000000000002e-05	Loss: 1.0589814841747285	
Step: 2050	Learning rate: 2.9521172638436482e-05	Loss: 1.0130263566970825	
Step: 2100	Learning rate: 2.903257328990228e-05	Loss: 0.9960906386375428	
Step: 2150	Learning rate: 2.854397394136808e-05	Loss: 0.9829663336277008	
Step: 2200	Learning rate: 2.8055374592833875e-05	Loss: 1.0640772151947022	
Step: 2250	Learning rate: 2.7566775244299674e-05	Loss: 1.0666393613815308	
Step: 2300	Learning rate: 2.7078175895765473e-05	Loss: 1.045463081598282	
Step: 2350	Learning rate: 2.658957654723127e-05	Loss: 0.972984322309494	
Step: 2400	Learning rate: 2.6100977198697067e-05	Loss: 0.965753219127655	
Step: 2450	Learning rate: 2.5612377850162865e-05	Loss: 0.9878271740674972	
Step: 2500	Learning rate: 2.5123778501628664e-05	Loss: 0.9699554085731507	
Step: 2550	Learning rate: 2.4635179153094466e-05	Loss: 0.9380973565578461	
Step: 2600	Learning rate: 2.414657980456026e-05	Loss: 0.937099483013153	
Step: 2650	Learning rate: 2.365798045602606e-05	Loss: 1.0179664027690887	
Step: 2700	Learning rate: 2.316938110749186e-05	Loss: 0.9129492461681366	
Step: 2750	Learning rate: 2.2680781758957654e-05	Loss: 0.974409728050232	
Step: 2800	Learning rate: 2.2192182410423453e-05	Loss: 0.961330052614212	
Step: 2850	Learning rate: 2.1703583061889252e-05	Loss: 0.9174932873249054	
Step: 2900	Learning rate: 2.121498371335505e-05	Loss: 0.8988187456130982	
Step: 2950	Learning rate: 2.0726384364820846e-05	Loss: 0.9141084408760071	
Step: 3000	Learning rate: 2.0237785016286645e-05	Loss: 1.0016864848136902	
Step: 3050	Learning rate: 1.9749185667752443e-05	Loss: 0.9803819692134857	
Step: 3100	Learning rate: 1.9260586319218242e-05	Loss: 0.9136419075727463	
Step: 3150	Learning rate: 1.8771986970684038e-05	Loss: 0.9638594055175781	
Step: 3200	Learning rate: 1.8283387622149836e-05	Loss: 0.940955001115799	
Step: 3250	Learning rate: 1.7794788273615635e-05	Loss: 0.8598595321178436	
Step: 3300	Learning rate: 1.7306188925081434e-05	Loss: 0.904008014202118	
Step: 3350	Learning rate: 1.6817589576547232e-05	Loss: 0.9198759770393372	
Step: 3400	Learning rate: 1.632899022801303e-05	Loss: 0.8894484877586365	
Step: 3450	Learning rate: 1.584039087947883e-05	Loss: 0.6810812455415726	
Step: 3500	Learning rate: 1.5351791530944625e-05	Loss: 0.6844633930921554	
Step: 3550	Learning rate: 1.4863192182410424e-05	Loss: 0.7082646632194519	
Step: 3600	Learning rate: 1.4374592833876223e-05	Loss: 0.7621362382173538	
Step: 3650	Learning rate: 1.388599348534202e-05	Loss: 0.6929214459657669	
Step: 3700	Learning rate: 1.3397394136807818e-05	Loss: 0.7318386542797088	
Step: 3750	Learning rate: 1.2908794788273615e-05	Loss: 0.7275706881284714	
Step: 3800	Learning rate: 1.2420195439739414e-05	Loss: 0.7118900352716446	
Step: 3850	Learning rate: 1.1931596091205211e-05	Loss: 0.6898960208892823	
Step: 3900	Learning rate: 1.144299674267101e-05	Loss: 0.7309998613595963	
Step: 3950	Learning rate: 1.0954397394136809e-05	Loss: 0.7209209948778152	
Step: 4000	Learning rate: 1.0465798045602606e-05	Loss: 0.6890686333179474	
Step: 4050	Learning rate: 9.977198697068404e-06	Loss: 0.6880157518386841	
Step: 4100	Learning rate: 9.488599348534201e-06	Loss: 0.6955315297842026	
Step: 4150	Learning rate: 9e-06	Loss: 0.7160925745964051	
Step: 4200	Learning rate: 8.511400651465797e-06	Loss: 0.6500678265094757	
Step: 4250	Learning rate: 8.022801302931596e-06	Loss: 0.7056504946947098	
Step: 4300	Learning rate: 7.534201954397395e-06	Loss: 0.6648940199613571	
Step: 4350	Learning rate: 7.045602605863192e-06	Loss: 0.6844071060419082	
Step: 4400	Learning rate: 6.5570032573289905e-06	Loss: 0.6962323838472366	
Step: 4450	Learning rate: 6.068403908794788e-06	Loss: 0.644142878651619	
Step: 4500	Learning rate: 5.579804560260586e-06	Loss: 0.6545238715410232	
Step: 4550	Learning rate: 5.091205211726384e-06	Loss: 0.635827077627182	
Step: 4600	Learning rate: 4.602605863192183e-06	Loss: 0.6153148871660232	
Step: 4650	Learning rate: 4.114006514657981e-06	Loss: 0.6743693852424621	
Step: 4700	Learning rate: 3.6254071661237786e-06	Loss: 0.6459405854344368	
Step: 4750	Learning rate: 3.1368078175895765e-06	Loss: 0.6664381009340287	
Step: 4800	Learning rate: 2.648208469055375e-06	Loss: 0.6682557547092438	
Step: 4850	Learning rate: 2.1596091205211727e-06	Loss: 0.5730735212564468	
Step: 4900	Learning rate: 1.6710097719869708e-06	Loss: 0.6955937683582306	
Step: 4950	Learning rate: 1.1824104234527687e-06	Loss: 0.6489062315225601	
Step: 5000	Learning rate: 6.938110749185669e-07	Loss: 0.621954203248024	
Saving model checkpoint to bert-output-bilstm/checkpoint-5000
Saving optimizer and scheduler states to bert-output-bilstm/checkpoint-5000
Step: 5050	Learning rate: 2.0521172638436482e-07	Loss: 0.6107679754495621	
 global_step = 5071, average loss = 1.262264435785782
Saving model to bert-output-bilstm/
Loading checkpointlogger.setLevel(logging.DEBUG)s saved during training for evaluation
Evaluate the following checkpoints: ['bert-output-bilstm/']
Creating features from dataset file at data/raw_data_cn_v3_sub
Saving features into cached file data/raw_data_cn_v3_sub/cached_dev_bert-base-chinese_512
***** Running evaluation  *****
  Num examples = 13405
  Batch size = 256
  Evaluation done in total 142.260993 secs (0.010613 sec per example)
Results: {'儿童故事': {'em': 9.2, 'f1': 25.9, 'turns': 3213}, '热点新闻': {'em': 14.4, 'f1': 52.5, 'turns': 813}, '历史': {'em': 14.2, 'f1': 45.2, 'turns': 1168}, '育儿知识': {'em': 1.0, 'f1': 8.1, 'turns': 310}, 'in_domain': {'em': 10.6, 'f1': 32.9, 'turns': 5504}, 'out_domain': {'em': 0.0, 'f1': 0.0, 'turns': 0}, 'overall': {'em': 10.6, 'f1': 32.9, 'turns': 5504}}
```

### RTransformers

```shell
python run_coqa.py --model_type bert \
                   --model_name_or_path /home/aistudio/bert_coqa/transformers-coqa-cn/bert-base-chinese \
                   --do_train \
                   --do_eval \
                   --data_dir data/raw_data_cn_v3_sub \
                   --train_file ConvQA_CN_v3.0_train.json \
                   --predict_file ConvQA_CN_v3.0_dev.json \
                   --max_seq_length 512 \
                   --doc_stride 384 \
                   --learning_rate 3e-5 \
                   --num_train_epochs 3 \
                   --output_dir bert-output-rtransformers-1/ \
                   --do_lower_case \
                   --per_gpu_train_batch_size 16  \
                   --per_gpu_eval_batch_size 128 \
                   --gradient_accumulation_steps 2 \
                   --max_grad_norm -1 \
                   --threads 4 \
                   --weight_decay 0.01
```

#### 日志，总的$F_1$得分为33.4

```shell
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False
Training/evaluation parameters Namespace(adam_epsilon=1e-08, adversarial=False, cache_dir='', config_name='', data_dir='data/raw_data_cn_v3_sub', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=True, doc_stride=384, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=2, history_len=2, learning_rate=3e-05, local_rank=-1, logging_steps=50, max_answer_length=30, max_grad_norm=-1.0, max_query_length=64, max_seq_length=512, max_steps=-1, model_name_or_path='/home/aistudio/bert_coqa/transformers-coqa-cn/bert-base-chinese', model_type='bert', n_best_size=20, n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='bert-output-rtransformers/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=128, per_gpu_train_batch_size=16, predict_file='ConvQA_CN_v3.0_dev.json', save_steps=5000, seed=42, server_ip='', server_port='', threads=4, tokenizer_name='', train_file='ConvQA_CN_v3.0_train.json', verbose_logging=False, warmup_steps=2000, weight_decay=0.01)
Total parameter number: 125246727
Creating features from dataset file at data/raw_data_cn_v3_sub
Saving features into cached file data/raw_data_cn_v3_sub/cached_train_bert-base-chinese_512
***** Running training *****
  Num examples = 54054
  Num Epochs = 3
  Instantaneous batch size per GPU = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 5067
  Starting fine-tuning.
Step: 50	Learning rate: 7.350000000000001e-07	Loss: 5.8848355102539065	
Step: 100	Learning rate: 1.4850000000000002e-06	Loss: 5.628987307548523	
Step: 150	Learning rate: 2.2349999999999998e-06	Loss: 4.944482107162475	
Step: 200	Learning rate: 2.9850000000000002e-06	Loss: 4.032596590518952	
Step: 250	Learning rate: 3.7350000000000002e-06	Loss: 3.237540609836578	
Step: 300	Learning rate: 4.485e-06	Loss: 2.6919539654254914	
Step: 350	Learning rate: 5.235e-06	Loss: 2.438988689184189	
Step: 400	Learning rate: 5.985e-06	Loss: 2.4881196141242983	
Step: 450	Learning rate: 6.735000000000001e-06	Loss: 2.4318118500709534	
Step: 500	Learning rate: 7.485e-06	Loss: 2.3691143310070037	
Step: 550	Learning rate: 8.235000000000002e-06	Loss: 2.2971922504901885	
Step: 600	Learning rate: 8.985e-06	Loss: 2.15248673081398	
Step: 650	Learning rate: 9.735e-06	Loss: 2.050472927093506	
Step: 700	Learning rate: 1.0485e-05	Loss: 1.8598755353689194	
Step: 750	Learning rate: 1.1235e-05	Loss: 1.8778807306289673	
Step: 800	Learning rate: 1.1985000000000001e-05	Loss: 1.884590008854866	
Step: 850	Learning rate: 1.2735e-05	Loss: 1.7552617079019546	
Step: 900	Learning rate: 1.3485e-05	Loss: 1.607119386792183	
Step: 950	Learning rate: 1.4235e-05	Loss: 1.6501935815811157	
Step: 1000	Learning rate: 1.4985e-05	Loss: 1.5885450446605682	
Step: 1050	Learning rate: 1.5735e-05	Loss: 1.354291586279869	
Step: 1100	Learning rate: 1.6485e-05	Loss: 1.4438664954900742	
Step: 1150	Learning rate: 1.7235e-05	Loss: 1.2941758358478546	
Step: 1200	Learning rate: 1.7985e-05	Loss: 1.300983093380928	
Step: 1250	Learning rate: 1.8735000000000003e-05	Loss: 1.2658867651224137	
Step: 1300	Learning rate: 1.9485e-05	Loss: 1.273248240351677	
Step: 1350	Learning rate: 2.0235e-05	Loss: 1.190572286248207	
Step: 1400	Learning rate: 2.0985e-05	Loss: 1.208802958726883	
Step: 1450	Learning rate: 2.1735e-05	Loss: 1.160976639688015	
Step: 1500	Learning rate: 2.2485000000000002e-05	Loss: 1.1524722844362258	
Step: 1550	Learning rate: 2.3235e-05	Loss: 1.0925501319766044	
Step: 1600	Learning rate: 2.3985e-05	Loss: 1.1718947571516036	
Step: 1650	Learning rate: 2.4735e-05	Loss: 1.072166881263256	
Step: 1700	Learning rate: 2.5485e-05	Loss: 1.1656985369324684	
Step: 1750	Learning rate: 2.6235000000000002e-05	Loss: 1.0480030229687691	
Step: 1800	Learning rate: 2.6985e-05	Loss: 1.0799457514286042	
Step: 1850	Learning rate: 2.7735e-05	Loss: 1.0515324592590332	
Step: 1900	Learning rate: 2.8485000000000003e-05	Loss: 1.0147670844197274	
Step: 1950	Learning rate: 2.9235e-05	Loss: 1.0833978551626204	
Step: 2000	Learning rate: 2.9985000000000002e-05	Loss: 1.0401951774954796	
Step: 2050	Learning rate: 2.9520704271274862e-05	Loss: 0.9928190952539444	
Step: 2100	Learning rate: 2.9031626997065538e-05	Loss: 0.9429317432641983	
Step: 2150	Learning rate: 2.854254972285621e-05	Loss: 0.9802166417241096	
Step: 2200	Learning rate: 2.8053472448646886e-05	Loss: 1.0318143913149833	
Step: 2250	Learning rate: 2.756439517443756e-05	Loss: 1.034688145518303	
Step: 2300	Learning rate: 2.7075317900228237e-05	Loss: 1.0132930305600167	
Step: 2350	Learning rate: 2.658624062601891e-05	Loss: 0.9526473240554333	
Step: 2400	Learning rate: 2.6097163351809585e-05	Loss: 0.9454410964250565	
Step: 2450	Learning rate: 2.560808607760026e-05	Loss: 0.9351805433630943	
Step: 2500	Learning rate: 2.5119008803390936e-05	Loss: 0.9520317521691323	
Step: 2550	Learning rate: 2.4629931529181612e-05	Loss: 0.9215599453449249	
Step: 2600	Learning rate: 2.4140854254972288e-05	Loss: 0.8929556798934937	
Step: 2650	Learning rate: 2.3651776980762963e-05	Loss: 0.9941600799560547	
Step: 2700	Learning rate: 2.3162699706553636e-05	Loss: 0.8825976321101189	
Step: 2750	Learning rate: 2.267362243234431e-05	Loss: 0.9610914742946625	
Step: 2800	Learning rate: 2.2184545158134987e-05	Loss: 0.9087370648980141	
Step: 2850	Learning rate: 2.169546788392566e-05	Loss: 0.9091071307659149	
Step: 2900	Learning rate: 2.1206390609716335e-05	Loss: 0.8699302479624749	
Step: 2950	Learning rate: 2.071731333550701e-05	Loss: 0.8896432545781136	
Step: 3000	Learning rate: 2.0228236061297686e-05	Loss: 0.9590858286619186	
Step: 3050	Learning rate: 1.973915878708836e-05	Loss: 0.9448389810323715	
Step: 3100	Learning rate: 1.9250081512879034e-05	Loss: 0.9034381115436554	
Step: 3150	Learning rate: 1.876100423866971e-05	Loss: 0.9447406959533692	
Step: 3200	Learning rate: 1.8271926964460382e-05	Loss: 0.9008041736483574	
Step: 3250	Learning rate: 1.7782849690251058e-05	Loss: 0.8248346430063248	
Step: 3300	Learning rate: 1.7293772416041737e-05	Loss: 0.8716826316714287	
Step: 3350	Learning rate: 1.6804695141832413e-05	Loss: 0.8702814128994941	
Step: 3400	Learning rate: 1.6315617867623085e-05	Loss: 0.8548263013362885	
Step: 3450	Learning rate: 1.582654059341376e-05	Loss: 0.6561028926074505	
Step: 3500	Learning rate: 1.5337463319204436e-05	Loss: 0.6643103951215744	
Step: 3550	Learning rate: 1.484838604499511e-05	Loss: 0.6775928276777268	
Step: 3600	Learning rate: 1.4359308770785784e-05	Loss: 0.7184291785955429	
Step: 3650	Learning rate: 1.387023149657646e-05	Loss: 0.6643050925433636	
Step: 3700	Learning rate: 1.3381154222367134e-05	Loss: 0.717695637345314	
Step: 3750	Learning rate: 1.289207694815781e-05	Loss: 0.6799249887466431	
Step: 3800	Learning rate: 1.2402999673948483e-05	Loss: 0.643797170817852	
Step: 3850	Learning rate: 1.1913922399739159e-05	Loss: 0.6355160053819418	
Step: 3900	Learning rate: 1.1424845125529835e-05	Loss: 0.701603211760521	
Step: 3950	Learning rate: 1.0935767851320509e-05	Loss: 0.6852636633813382	
Step: 4000	Learning rate: 1.0446690577111184e-05	Loss: 0.6361671294271946	
Step: 4050	Learning rate: 9.957613302901858e-06	Loss: 0.6746354486048222	
Step: 4100	Learning rate: 9.468536028692534e-06	Loss: 0.6315448075532913	
Step: 4150	Learning rate: 8.979458754483208e-06	Loss: 0.6598882372677326	
Step: 4200	Learning rate: 8.490381480273882e-06	Loss: 0.6334964855015278	
Step: 4250	Learning rate: 8.00130420606456e-06	Loss: 0.6325242453813553	
Step: 4300	Learning rate: 7.5122269318552325e-06	Loss: 0.6242368921637536	
Step: 4350	Learning rate: 7.023149657645908e-06	Loss: 0.634989588111639	
Step: 4400	Learning rate: 6.534072383436583e-06	Loss: 0.647121774405241	
Step: 4450	Learning rate: 6.044995109227258e-06	Loss: 0.618138107061386	
Step: 4500	Learning rate: 5.5559178350179326e-06	Loss: 0.5999011455476284	
Step: 4550	Learning rate: 5.066840560808608e-06	Loss: 0.5719111227244139	
Step: 4600	Learning rate: 4.577763286599283e-06	Loss: 0.5732899771630764	
Step: 4650	Learning rate: 4.088686012389958e-06	Loss: 0.6260074231028557	
Step: 4700	Learning rate: 3.5996087381806327e-06	Loss: 0.6067899906635285	
Step: 4750	Learning rate: 3.1105314639713075e-06	Loss: 0.6031355957686901	
Step: 4800	Learning rate: 2.6214541897619823e-06	Loss: 0.6108962285518647	
Step: 4850	Learning rate: 2.132376915552657e-06	Loss: 0.5557198016345501	
Step: 4900	Learning rate: 1.6432996413433324e-06	Loss: 0.6302701959013939	
Step: 4950	Learning rate: 1.1542223671340072e-06	Loss: 0.5916951155662536	
Step: 5000	Learning rate: 6.651450929246821e-07	Loss: 0.5495449204742908	
Saving model checkpoint to bert-output-rtransformers/checkpoint-5000
Saving optimizer and scheduler states to bert-output-rtransformers/checkpoint-5000
Step: 5050	Learning rate: 1.7606781871535703e-07	Loss: 0.585224674642086	
 global_step = 5068, average loss = 1.2386860071998251
Saving model to bert-output-rtransformers/
Loading checkpointlogger.setLevel(logging.DEBUG)s saved during training for evaluation
Evaluate the following checkpoints: ['bert-output-rtransformers/']
Creating features from dataset file at data/raw_data_cn_v3_sub
Saving features into cached file data/raw_data_cn_v3_sub/cached_dev_bert-base-chinese_512
***** Running evaluation  *****
  Num examples = 13405
  Batch size = 128
  Evaluation done in total 144.239597 secs (0.010760 sec per example)
Results: {'儿童故事': {'em': 9.2, 'f1': 26.8, 'turns': 3213}, '热点新闻': {'em': 14.1, 'f1': 53.3, 'turns': 813}, '历史': {'em': 14.6, 'f1': 44.6, 'turns': 1168}, '育儿知识': {'em': 1.0, 'f1': 8.1, 'turns': 310}, 'in_domain': {'em': 10.6, 'f1': 33.4, 'turns': 5504}, 'out_domain': {'em': 0.0, 'f1': 0.0, 'turns': 0}, 'overall': {'em': 10.6, 'f1': 33.4, 'turns': 5504}}
```

## 生成式模型微调

### 修改验证集之前

```shell
export OUTPUT_DIR="Experiments/QAQAFQ_CN_1"
# export CACHE_DIR=transformer_package_cache
# shellcheck disable=SC2034
CUDA_VISIBLE_DEVICES=0
python run_seq2seq.py --train_file "ConvQA/QAQAFQ/ConvQA_train.json" \
                      --output_dir ${OUTPUT_DIR} \
                      --model_type unilm \
                      --model_name_or_path "C:\pretrained_model\unilm-cn-base/" \
                      --do_lower_case \
                      --max_source_seq_length 512 \
                      --max_target_seq_length 42 \
                      --per_gpu_train_batch_size 6 \
                      --gradient_accumulation_steps 4 \
                      --learning_rate 2e-5 \
                      --num_warmup_steps 2236 \
                      --num_training_epochs 15 \
                      --save_steps 77777
                      # --cache_dir $CACHE_DIR
                      # --fp16 --fp16_opt_level O2
```

- 总的$F_1$得分为35.8